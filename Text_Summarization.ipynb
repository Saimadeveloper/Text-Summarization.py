{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkZwrL0hmeR2lBduxi9xK4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saimadeveloper/Text-Summarization.py/blob/main/Text_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from textblob import TextBlob\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# Load SpaCy language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ],
      "metadata": {
        "id": "OdisGuREIJH2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Remove extra spaces and clean special characters\n",
        "    text = re.sub(r'\\s+', ' ', text.strip())\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "-lGoXR1aINyh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58bxkwl9HzNR",
        "outputId": "0b1f2413-dc16-45c2-a803-d47790bad00a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m textblob.download_corpora\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ivKIiQQH3Kw",
        "outputId": "7957599f-671d-4f05-865a-a54ed2c1804e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "print(nltk.data.find('tokenizers/punkt'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2LVV3lBIJtK",
        "outputId": "c59967de-db5b-449f-e312-726bd0df371f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/nltk_data/tokenizers/punkt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_sentences(text):\n",
        "    doc = nlp(text)\n",
        "    return [sent.text.strip() for sent in doc.sents]\n"
      ],
      "metadata": {
        "id": "m40MUpLyIRlV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import Word\n",
        "\n",
        "def get_keywords(text, top_n=10):\n",
        "    blob = TextBlob(text)\n",
        "    words = [Word(word.lower()) for word in blob.words if word.isalpha()]\n",
        "    words = [word for word in words if word not in nlp.Defaults.stop_words]\n",
        "    word_freq = Counter(words)\n",
        "    return [word.string for word, _ in word_freq.most_common(top_n)]\n"
      ],
      "metadata": {
        "id": "tb-RNQWCJWvc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_text(text, summary_ratio=0.4):\n",
        "    text = preprocess_text(text)\n",
        "    sentences = extract_sentences(text)\n",
        "    keywords = get_keywords(text)\n",
        "\n",
        "    sentence_scores = {}\n",
        "    for sentence in sentences:\n",
        "        sentence_lower = sentence.lower()\n",
        "        score = sum(1 for word in keywords if word in sentence_lower)\n",
        "\n",
        "        # Perform similarity check only if word vectors are available\n",
        "        doc_input = nlp(sentence_lower)\n",
        "        if nlp.vocab.has_vector:\n",
        "            score += sum(doc_input.similarity(nlp(word)) for word in keywords)\n",
        "\n",
        "        sentence_scores[sentence] = score\n",
        "\n",
        "    # Select the top sentences\n",
        "    sorted_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)\n",
        "    num_sentences = max(1, int(len(sentences) * summary_ratio))\n",
        "    summary = sorted_sentences[:num_sentences]\n",
        "\n",
        "    return ' '.join(summary)\n"
      ],
      "metadata": {
        "id": "MjAXUQcjKhBG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "Artificial Intelligence (AI) is rapidly transforming industries across the globe.\n",
        "From healthcare and finance to autonomous vehicles and cybersecurity, AI-powered systems are solving complex problems.\n",
        "Machine learning algorithms, particularly deep learning models, are making significant advancements.\n",
        "However, ethical concerns, data privacy issues, and the potential for job displacement remain significant challenges.\n",
        "Despite these challenges, AI continues to drive innovation and improve efficiencies across various sectors.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Original Text:\\n\", text)\n",
        "summary = summarize_text(text)\n",
        "print(\"\\nGenerated Summary:\\n\", summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySyiR-XdI8hz",
        "outputId": "5dd23729-c417-4683-f1fb-bb2e703b2d67"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            " \n",
            "Artificial Intelligence (AI) is rapidly transforming industries across the globe.\n",
            "From healthcare and finance to autonomous vehicles and cybersecurity, AI-powered systems are solving complex problems.\n",
            "Machine learning algorithms, particularly deep learning models, are making significant advancements.\n",
            "However, ethical concerns, data privacy issues, and the potential for job displacement remain significant challenges.\n",
            "Despite these challenges, AI continues to drive innovation and improve efficiencies across various sectors.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-d1e4367d20c1>:14: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  score += sum(doc_input.similarity(nlp(word)) for word in keywords)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Summary:\n",
            " Artificial Intelligence (AI) is rapidly transforming industries across the globe. However, ethical concerns, data privacy issues, and the potential for job displacement remain significant challenges.\n"
          ]
        }
      ]
    }
  ]
}